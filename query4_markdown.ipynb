{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import redis\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bank_53'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from counter import get_last_counter\n",
    "\n",
    "bank_id = f\"bank_{get_last_counter()}\"\n",
    "# bank_id = \"bank_52\"\n",
    "bank_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id_by_index', 'chunk_by_index', 'faiss_index'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4960"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from json import loads\n",
    "from IPython.display import display\n",
    "\n",
    "index = loads(r.get(f\"faiss_index:v1::{bank_id}\"))\n",
    "# Write index to JSON file\n",
    "import json\n",
    "with open('faiss_index.json', 'w') as f:\n",
    "    json.dump(index, f, indent=2)\n",
    "display(index.keys())\n",
    "display(len(index['id_by_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryDocumentsResponse(chunks=[Chunk(content='```\\n>>> from transformers import AutoConfig, FlaxAutoModelForCausalLM\\n\\n>>> \\n>>> model = FlaxAutoModelForCausalLM.from\\\\_pretrained(\"bert-base-cased\")\\n\\n>>> \\n>>> model = FlaxAutoModelForCausalLM.from\\\\_pretrained(\"bert-base-cased\", output\\\\_attentions=True)\\n>>> model.config.output\\\\_attentions\\nTrue\\n\\n>>> \\n>>> config = AutoConfig.from\\\\_pretrained(\"./pt\\\\_model/bert\\\\_pt\\\\_model\\\\_config.json\")\\n>>> model = FlaxAutoModelForCausalLM.from\\\\_pretrained(\\n...     \"./pt\\\\_model/bert\\\\_pytorch\\\\_model.bin\", from\\\\_pt=True, config=config\\n... )\\n```\\n\\nAutoModelForMaskedLM\\n\\nclass transformers.AutoModelForMaskedLM\\n\\n< source\\n\\n( argskwargs )\\n\\nThis is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created with the frompretrained() class method or the fromconfig() class method.\\n\\nThis class cannot be instantiated directly using\\n\\n```\\n\\\\_\\\\_init\\\\_\\\\_()\\n```\\n\\n(throws an error).\\n\\nInstantiates one of the model classes of the library (with a masked language modeling head) from a configuration.\\n\\nNote: Loading a model from its configuration file does not load the model weights. It only affects the model’s configuration. Use frompretrained() to load the model weights.\\n\\nExamples:\\n\\n```\\n>>> from transformers import AutoConfig, AutoModelForMaskedLM\\n\\n>>> \\n>>> config = AutoConfig.from\\\\_pretrained(\"bert-base-cased\")\\n>>> model = AutoModelForMaskedLM.from\\\\_config(config)\\n```\\n\\nTFAutoModelForMaskedLM\\n\\nclass transformers.TFAutoModelForMaskedLM\\n\\n< source\\n\\n( argskwargs )\\n\\nThis is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created with the frompretrained() class method or the fromconfig() class method.\\n\\nThis class cannot be instantiated directly using\\n\\n```\\n\\\\_\\\\_init\\\\_\\\\_()\\n```\\n\\n(throws an error).\\n\\nInstantiates one of the model classes of the library (with a masked language modeling head) from a configuration.\\n\\nNote: Loading a model from its configuration file does not load the model weights. It only affects the model’s configuration. Use frompretrained() to load the model weights.\\n\\nExamples:\\n\\n```\\n>>> from transformers import AutoConfig,', document_id='70', token_count=512), Chunk(content='.FlaubertForMultipleChoice\\n\\n< source\\n\\n( configinputskwargs )\\n\\nParameters\\n\\n [<RawText children=\\'config\\'>]\\n\\nFlaubert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.\\n\\nThis model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\\n\\nThis model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\\n\\nforward\\n\\n< source\\n\\n( inputids: typing.Optionaltorch.Tensor = Noneattentionmask: typing.Optionaltorch.Tensor = Nonelangs: typing.Optionaltorch.Tensor = Nonetokentypeids: typing.Optionaltorch.Tensor = Nonepositionids: typing.Optionaltorch.Tensor = Nonelengths: typing.Optionaltorch.Tensor = Nonecache: typing.Uniontyping.Dictstr, torch.Tensor, NoneType = Noneheadmask: typing.Optionaltorch.Tensor = Noneinputsembeds: typing.Optionaltorch.Tensor = Nonelabels: typing.Optionaltorch.Tensor = Noneoutputattentions: typing.Optionalbool = Noneoutputhiddenstates: typing.Optionalbool = Nonereturndict: typing.Optionalbool = None ) → transformers.modelingoutputs.MultipleChoiceModelOutput or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nThe FlaubertForMultipleChoice forward method, overrides the\\n\\n```\\n\\\\_\\\\_call\\\\_\\\\_\\n```\\n\\nspecial method.\\n\\nAlthough the recipe for forward pass needs to be defined within this function, one should call the\\n\\n```\\nModule\\n```\\n\\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\\n\\nExample:\\n\\n```\\n>>> from transformers import AutoTokenizer, FlaubertForMultipleChoice\\n>>> import torch\\n\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"flaubert/flaubert\\\\_base\\\\_cased\")\\n>>> model = FlaubertForMultipleChoice.from\\\\_pretrained(\"flaubert/flaubert\\\\_base\\\\_cased\")\\n\\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\\n>>> choice0 = \"It is eaten with a fork and a knife.\"\\n>>> choice1 = \"It is eaten while held in the hand.\"\\n>>> labels', document_id='165', token_count=512), Chunk(content=\" None outputhiddenstates: typing.Optionalbool = None returndict: typing.Optionalbool = None train: bool = False params: dict = None pastkeyvalues: dict = None dropoutrng: PRNGKey = None ) → transformers.modelingflaxoutputs.FlaxBaseModelOutputWithPastAndCrossAttentions or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nParameters\\n\\n [<RawText children='input'>, <Literal children='\\\\_'>, <RawText children='ids'>]\\n\\n [<RawText children='attention'>, <Literal children='\\\\_'>, <RawText children='mask'>]\\n\\n [<RawText children='position'>, <Literal children='\\\\_'>, <RawText children='ids'>]\\n\\n [<RawText children='output'>, <Literal children='\\\\_'>, <RawText children='attentions'>]\\n\\n [<RawText children='output'>, <Literal children='\\\\_'>, <RawText children='hidden'>, <Literal children='\\\\_'>, <RawText children='states'>]\\n\\n [<RawText children='return'>, <Literal children='\\\\_'>, <RawText children='dict'>]\\n\\nA transformers.modelingflaxoutputs.FlaxBaseModelOutputWithPastAndCrossAttentions or a tuple of\\n\\n```\\ntorch.FloatTensor\\n```\\n\\n(if\\n\\n```\\nreturn\\\\_dict=False\\n```\\n\\nis passed or when\\n\\n```\\nconfig.return\\\\_dict=False\\n```\\n\\n) comprising various elements depending on the configuration (XGLMConfig) and inputs.\\n\\n [<RawText children='last'>, <Literal children='\\\\_'>, <RawText children='hidden'>, <Literal children='\\\\_'>, <RawText children='state'>]\\n\\n [<RawText children='past'>, <Literal children='\\\\_'>, <RawText children='key'>, <Literal children='\\\\_'>, <RawText children='values'>]\\n\\n [<RawText children='hidden'>, <Literal children='\\\\_'>, <RawText children='states'>]\\n\\n [<RawText children='attentions'>]\\n\\n [<RawText children='cross'>, <Literal children='\\\\_'>, <RawText children='attentions'>]\\n\\nThe\\n\\n```\\nFlaxXGLMPreTrainedModel\\n```\\n\\nforward method, overrides the\\n\\n```\\n\\\\_\\\\_call\\\\_\\\\_\\n```\\n\\nspecial method.\\n\\nAlthough the recipe for forward pass needs to be defined within this function, one should call the\\n\\n```\\nModule\\n```\\n\\ninstance afterwards instead of this since the former takes care of running the pre\", document_id='309', token_count=512), Chunk(content=' <Literal children=\\'\\\\_\\'>, <RawText children=\\'attentions\\'>]\\n\\n [<RawText children=\\'output\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'states\\'>]\\n\\n [<RawText children=\\'return\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'dict\\'>]\\n\\nA transformers.modelingflaxoutputs.FlaxBaseModelOutputWithPastAndCrossAttentions or a tuple of\\n\\n```\\ntorch.FloatTensor\\n```\\n\\n(if\\n\\n```\\nreturn\\\\_dict=False\\n```\\n\\nis passed or when\\n\\n```\\nconfig.return\\\\_dict=False\\n```\\n\\n) comprising various elements depending on the configuration (GPT2Config) and inputs.\\n\\n [<RawText children=\\'last\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'state\\'>]\\n\\n [<RawText children=\\'past\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'key\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'values\\'>]\\n\\n [<RawText children=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'states\\'>]\\n\\n [<RawText children=\\'attentions\\'>]\\n\\n [<RawText children=\\'cross\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'attentions\\'>]\\n\\nThe\\n\\n```\\nFlaxGPT2PreTrainedModel\\n```\\n\\nforward method, overrides the\\n\\n```\\n\\\\_\\\\_call\\\\_\\\\_\\n```\\n\\nspecial method.\\n\\nAlthough the recipe for forward pass needs to be defined within this function, one should call the\\n\\n```\\nModule\\n```\\n\\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\\n\\nExample:\\n\\n```\\n>>> from transformers import AutoTokenizer, FlaxGPT2Model\\n\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"gpt2\")\\n>>> model = FlaxGPT2Model.from\\\\_pretrained(\"gpt2\")\\n\\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return\\\\_tensors=\"jax\")\\n>>> outputs = model(**inputs)\\n\\n>>> last\\\\_hidden\\\\_states = outputs.last\\\\_hidden\\\\_state\\n```\\n\\nFlaxGPT2LMHeadModel\\n\\nclass transformers.FlaxGPT2LMHeadModel\\n\\n< source\\n\\n( config: GPT2Config inputshape: typing.Tuple = (1, 1) seed: int = 0 dtype: dtype', document_id='173', token_count=512), Chunk(content='cased\")\\n>>> model = FlaubertModel.from\\\\_pretrained(\"flaubert/flaubert\\\\_base\\\\_cased\")\\n\\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return\\\\_tensors=\"pt\")\\n>>> outputs = model(**inputs)\\n\\n>>> last\\\\_hidden\\\\_states = outputs.last\\\\_hidden\\\\_state\\n```\\n\\nFlaubertWithLMHeadModel\\n\\nclass transformers.FlaubertWithLMHeadModel\\n\\n< source\\n\\n( config )\\n\\nParameters\\n\\n [<RawText children=\\'config\\'>]\\n\\nThe Flaubert Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\\n\\nThis model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\\n\\nThis model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\\n\\nforward\\n\\n< source\\n\\n( inputids: typing.Optionaltorch.Tensor = Noneattentionmask: typing.Optionaltorch.Tensor = Nonelangs: typing.Optionaltorch.Tensor = Nonetokentypeids: typing.Optionaltorch.Tensor = Nonepositionids: typing.Optionaltorch.Tensor = Nonelengths: typing.Optionaltorch.Tensor = Nonecache: typing.Uniontyping.Dictstr, torch.Tensor, NoneType = Noneheadmask: typing.Optionaltorch.Tensor = Noneinputsembeds: typing.Optionaltorch.Tensor = Nonelabels: typing.Optionaltorch.Tensor = Noneoutputattentions: typing.Optionalbool = Noneoutputhiddenstates: typing.Optionalbool = Nonereturndict: typing.Optionalbool = None ) → transformers.modelingoutputs.MaskedLMOutput or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nThe FlaubertWithLMHeadModel forward method, overrides the\\n\\n```\\n\\\\_\\\\_call\\\\_\\\\_\\n```\\n\\nspecial method.\\n\\nAlthough the recipe for forward pass needs to be defined within this function, one should call the\\n\\n```\\nModule\\n```\\n\\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\\n\\nExample:\\n\\n```\\n>>> from transformers import AutoTokenizer, FlaubertWithLMHeadModel\\n>>> import torch\\n\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"flaubert/flaubert\\\\_base\\\\_cased\")\\n>>> model = FlaubertWithLMHeadModel.from\\\\_pretrained(\"flaub', document_id='165', token_count=512), Chunk(content='=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'states\\'>]\\n\\n [<RawText children=\\'return\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'dict\\'>]\\n\\nA transformers.modelingflaxoutputs.FlaxBaseModelOutput or a tuple of\\n\\n```\\ntorch.FloatTensor\\n```\\n\\n(if\\n\\n```\\nreturn\\\\_dict=False\\n```\\n\\nis passed or when\\n\\n```\\nconfig.return\\\\_dict=False\\n```\\n\\n) comprising various elements depending on the configuration (\\n\\n```\\n<class \\'transformers.models.longt5.configuration\\\\_longt5.LongT5Config\\'>\\n```\\n\\n) and inputs.\\n\\n [<RawText children=\\'last\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'state\\'>]\\n\\n [<RawText children=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'states\\'>]\\n\\n [<RawText children=\\'attentions\\'>]\\n\\nExample:\\n\\n```\\n>>> from transformers import AutoTokenizer, FlaxLongT5ForConditionalGeneration\\n\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"t5-base\")\\n>>> model = FlaxLongT5ForConditionalGeneration.from\\\\_pretrained(\"google/long-t5-local-base\")\\n\\n>>> text = \"My friends are cool but they eat too many carbs.\"\\n>>> inputs = tokenizer(text, return\\\\_tensors=\"np\")\\n>>> encoder\\\\_outputs = model.encode(**inputs)\\n```\\n\\ndecode\\n\\n< source\\n\\n( decoderinputids encoderoutputs encoderattentionmask: typing.Optionaljax.Array = None decoderattentionmask: typing.Optionaljax.Array = None pastkeyvalues: dict = None outputattentions: typing.Optionalbool = None outputhiddenstates: typing.Optionalbool = None returndict: typing.Optionalbool = None train: bool = False params: dict = None dropoutrng: PRNGKey = None ) → transformers.modelingflaxoutputs.FlaxBaseModelOutputWithPastAndCrossAttentions or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nParameters\\n\\n [<RawText children=\\'decoder\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'input\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'ids\\'>]\\n\\n [<RawText children=\\'encoder\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'outputs\\'>]\\n\\n [<RawText children=\\'encoder\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'attention\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'mask\\'>]\\n\\n [<', document_id='198', token_count=512), Chunk(content='\\\\_ids[0]]\\n\\n>>> labels = predicted\\\\_token\\\\_class\\\\_ids\\n>>> loss = model(**inputs, labels=labels).loss\\n```\\n\\nFlaubertForQuestionAnsweringSimple\\n\\nclass transformers.FlaubertForQuestionAnsweringSimple\\n\\n< source\\n\\n( config )\\n\\nParameters\\n\\n [<RawText children=\\'config\\'>]\\n\\nFlaubert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of the hidden-states output to compute\\n\\n```\\nspan start logits\\n```\\n\\nand\\n\\n```\\nspan end logits\\n```\\n\\n).\\n\\nThis model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\\n\\nThis model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\\n\\nforward\\n\\n< source\\n\\n( inputids: typing.Optionaltorch.Tensor = Noneattentionmask: typing.Optionaltorch.Tensor = Nonelangs: typing.Optionaltorch.Tensor = Nonetokentypeids: typing.Optionaltorch.Tensor = Nonepositionids: typing.Optionaltorch.Tensor = Nonelengths: typing.Optionaltorch.Tensor = Nonecache: typing.Uniontyping.Dictstr, torch.Tensor, NoneType = Noneheadmask: typing.Optionaltorch.Tensor = Noneinputsembeds: typing.Optionaltorch.Tensor = Nonestartpositions: typing.Optionaltorch.Tensor = Noneendpositions: typing.Optionaltorch.Tensor = Noneoutputattentions: typing.Optionalbool = Noneoutputhiddenstates: typing.Optionalbool = Nonereturndict: typing.Optionalbool = None ) → transformers.modelingoutputs.QuestionAnsweringModelOutput or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nThe FlaubertForQuestionAnsweringSimple forward method, overrides the\\n\\n```\\n\\\\_\\\\_call\\\\_\\\\_\\n```\\n\\nspecial method.\\n\\nAlthough the recipe for forward pass needs to be defined within this function, one should call the\\n\\n```\\nModule\\n```\\n\\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\\n\\nExample:\\n\\n```\\n>>> from transformers import AutoTokenizer, FlaubertForQuestionAnsweringSimple\\n>>> import torch\\n\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"flaubert/flaubert\\\\_base\\\\_cased\")\\n>>> model = Flaub', document_id='165', token_count=512), Chunk(content='trained(\"junnyu/roformer\\\\_chinese\\\\_base\")\\n>>> model = FlaxRoFormerForMaskedLM.from\\\\_pretrained(\"junnyu/roformer\\\\_chinese\\\\_base\")\\n\\n>>> inputs = tokenizer(\"The capital of France is [MASK].\", return\\\\_tensors=\"jax\")\\n\\n>>> outputs = model(**inputs)\\n>>> logits = outputs.logits\\n```\\n\\nFlaxRoFormerForSequenceClassification\\n\\nclass transformers.FlaxRoFormerForSequenceClassification\\n\\n< source\\n\\n( config: RoFormerConfig inputshape: typing.Tuple = (1, 1) seed: int = 0 dtype: dtype = <class \\'jax.numpy.float32\\'> doinit: bool = True kwargs )\\n\\nParameters\\n\\n [<RawText children=\\'config\\'>]\\n\\n [<RawText children=\\'dtype\\'>]\\n\\nRoFormer Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.\\n\\nThis model inherits from FlaxPreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading, saving and converting weights from PyTorch models)\\n\\nThis model is also a Flax Linen flax.linen.Module subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and behavior.\\n\\nFinally, this model supports inherent JAX features such as:\\n\\n [<RawText children=\\'Just-In-Time (JIT) compilation\\'>]\\n\\n [<RawText children=\\'Automatic Differentiation\\'>]\\n\\n [<RawText children=\\'Vectorization\\'>]\\n\\n [<RawText children=\\'Parallelization\\'>]\\n\\n\\\\_\\\\_call\\\\_\\\\_\\n\\n< source\\n\\n( inputids attentionmask = None tokentypeids = None headmask = None params: dict = None dropoutrng: PRNGKey = None train: bool = False outputattentions: typing.Optionalbool = None outputhiddenstates: typing.Optionalbool = None returndict: typing.Optionalbool = None ) → transformers.modelingflaxoutputs.FlaxSequenceClassifierOutput or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nParameters\\n\\n [<RawText children=\\'input\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'ids\\'>]\\n\\n [<RawText children=\\'attention\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'mask\\'>]\\n\\n [<RawText children=\\'token\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'type\\'>, <Literal children=\\'\\\\_\\'>, <RawText', document_id='256', token_count=512), Chunk(content=' or loaded from\\n\\n```\\npretrained\\\\_model\\\\_name\\\\_or\\\\_path\\n```\\n\\nif possible), or when it’s missing, by falling back to using pattern matching on\\n\\n```\\npretrained\\\\_model\\\\_name\\\\_or\\\\_path\\n```\\n\\n:\\n\\n [<RawText children=\\'beit\\'>]\\n\\n [<RawText children=\\'regnet\\'>]\\n\\n [<RawText children=\\'resnet\\'>]\\n\\n [<RawText children=\\'vit\\'>]\\n\\nExamples:\\n\\n```\\n>>> from transformers import AutoConfig, FlaxAutoModelForImageClassification\\n\\n>>> \\n>>> model = FlaxAutoModelForImageClassification.from\\\\_pretrained(\"bert-base-cased\")\\n\\n>>> \\n>>> model = FlaxAutoModelForImageClassification.from\\\\_pretrained(\"bert-base-cased\", output\\\\_attentions=True)\\n>>> model.config.output\\\\_attentions\\nTrue\\n\\n>>> \\n>>> config = AutoConfig.from\\\\_pretrained(\"./pt\\\\_model/bert\\\\_pt\\\\_model\\\\_config.json\")\\n>>> model = FlaxAutoModelForImageClassification.from\\\\_pretrained(\\n...     \"./pt\\\\_model/bert\\\\_pytorch\\\\_model.bin\", from\\\\_pt=True, config=config\\n... )\\n```\\n\\nAutoModelForVideoClassification\\n\\nclass transformers.AutoModelForVideoClassification\\n\\n< source\\n\\n( argskwargs )\\n\\nThis is a generic model class that will be instantiated as one of the model classes of the library (with a video classification head) when created with the frompretrained() class method or the fromconfig() class method.\\n\\nThis class cannot be instantiated directly using\\n\\n```\\n\\\\_\\\\_init\\\\_\\\\_()\\n```\\n\\n(throws an error).\\n\\nfrom\\\\_config\\n\\n< source\\n\\n( kwargs )\\n\\nParameters\\n\\n [<RawText children=\\'config\\'>]\\n\\nInstantiates one of the model classes of the library (with a video classification head) from a configuration.\\n\\nNote: Loading a model from its configuration file does not load the model weights. It only affects the model’s configuration. Use frompretrained() to load the model weights.\\n\\nExamples:\\n\\n```\\n>>> from transformers import AutoConfig, AutoModelForVideoClassification\\n\\n>>> \\n>>> config = AutoConfig.from\\\\_pretrained(\"bert-base-cased\")\\n>>> model = AutoModelForVideoClassification.from\\\\_config(config)\\n```\\n\\nInstantiate one of the model classes of the library (with a video classification head) from a pretrained model.\\n\\nThe model class to instantiate is selected based on the\\n\\n```\\nmodel\\\\_type\\n```\\n\\nproperty of the config object (either passed as an argument or loaded from\\n\\n```\\npretrained\\\\_model\\\\_name\\\\_or', document_id='70', token_count=512), Chunk(content=' attentionmask = None tokentypeids = None headmask = None params: dict = None dropoutrng: PRNGKey = None train: bool = False outputattentions: typing.Optionalbool = None outputhiddenstates: typing.Optionalbool = None returndict: typing.Optionalbool = None ) → transformers.modelingflaxoutputs.FlaxMultipleChoiceModelOutput or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nParameters\\n\\n [<RawText children=\\'input\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'ids\\'>]\\n\\n [<RawText children=\\'attention\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'mask\\'>]\\n\\n [<RawText children=\\'token\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'type\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'ids\\'>]\\n\\n [<RawText children=\\'position\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'ids\\'>]\\n\\n [<RawText children=\\'head\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'mask\\'>]\\n\\n [<RawText children=\\'return\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'dict\\'>]\\n\\nA transformers.modelingflaxoutputs.FlaxMultipleChoiceModelOutput or a tuple of\\n\\n```\\ntorch.FloatTensor\\n```\\n\\n(if\\n\\n```\\nreturn\\\\_dict=False\\n```\\n\\nis passed or when\\n\\n```\\nconfig.return\\\\_dict=False\\n```\\n\\n) comprising various elements depending on the configuration (RoFormerConfig) and inputs.\\n\\n [<RawText children=\\'logits\\'>]\\n\\n [<RawText children=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'states\\'>]\\n\\n [<RawText children=\\'attentions\\'>]\\n\\nThe\\n\\n```\\nFlaxRoFormerPreTrainedModel\\n```\\n\\nforward method, overrides the\\n\\n```\\n\\\\_\\\\_call\\\\_\\\\_\\n```\\n\\nspecial method.\\n\\nAlthough the recipe for forward pass needs to be defined within this function, one should call the\\n\\n```\\nModule\\n```\\n\\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\\n\\nExample:\\n\\n```\\n>>> from transformers import AutoTokenizer, FlaxRoFormerForMultipleChoice\\n\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"junnyu/roformer\\\\_chinese\\\\_base\")\\n>>> model = FlaxRoFormerForMultipleChoice.from\\\\_pretrained(\"junnyu/roformer\\\\_chinese\\\\_base\")\\n\\n>>> prompt = \"In Italy, pizza served in', document_id='256', token_count=512)], scores=[0.6564961486472527, 0.6455808621466147, 0.6438103442109543, 0.6374187536159951, 0.6366055990519301, 0.6356340892193422, 0.6318881208219916, 0.6313718645472836, 0.6303673622623528, 0.6253748398655467])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk(content='```\\n>>> from transformers import AutoConfig, FlaxAutoModelForCausalLM\\n\\n>>> \\n>>> model = FlaxAutoModelForCausalLM.from\\\\_pretrained(\"bert-base-cased\")\\n\\n>>> \\n>>> model = FlaxAutoModelForCausalLM.from\\\\_pretrained(\"bert-base-cased\", output\\\\_attentions=True)\\n>>> model.config.output\\\\_attentions\\nTrue\\n\\n>>> \\n>>> config = AutoConfig.from\\\\_pretrained(\"./pt\\\\_model/bert\\\\_pt\\\\_model\\\\_config.json\")\\n>>> model = FlaxAutoModelForCausalLM.from\\\\_pretrained(\\n...     \"./pt\\\\_model/bert\\\\_pytorch\\\\_model.bin\", from\\\\_pt=True, config=config\\n... )\\n```\\n\\nAutoModelForMaskedLM\\n\\nclass transformers.AutoModelForMaskedLM\\n\\n< source\\n\\n( argskwargs )\\n\\nThis is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created with the frompretrained() class method or the fromconfig() class method.\\n\\nThis class cannot be instantiated directly using\\n\\n```\\n\\\\_\\\\_init\\\\_\\\\_()\\n```\\n\\n(throws an error).\\n\\nInstantiates one of the model classes of the library (with a masked language modeling head) from a configuration.\\n\\nNote: Loading a model from its configuration file does not load the model weights. It only affects the model’s configuration. Use frompretrained() to load the model weights.\\n\\nExamples:\\n\\n```\\n>>> from transformers import AutoConfig, AutoModelForMaskedLM\\n\\n>>> \\n>>> config = AutoConfig.from\\\\_pretrained(\"bert-base-cased\")\\n>>> model = AutoModelForMaskedLM.from\\\\_config(config)\\n```\\n\\nTFAutoModelForMaskedLM\\n\\nclass transformers.TFAutoModelForMaskedLM\\n\\n< source\\n\\n( argskwargs )\\n\\nThis is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created with the frompretrained() class method or the fromconfig() class method.\\n\\nThis class cannot be instantiated directly using\\n\\n```\\n\\\\_\\\\_init\\\\_\\\\_()\\n```\\n\\n(throws an error).\\n\\nInstantiates one of the model classes of the library (with a masked language modeling head) from a configuration.\\n\\nNote: Loading a model from its configuration file does not load the model weights. It only affects the model’s configuration. Use frompretrained() to load the model weights.\\n\\nExamples:\\n\\n```\\n>>> from transformers import AutoConfig,', document_id='70', token_count=512)\n",
      "0.6564961486472527\n",
      "Chunk(content='.FlaubertForMultipleChoice\\n\\n< source\\n\\n( configinputskwargs )\\n\\nParameters\\n\\n [<RawText children=\\'config\\'>]\\n\\nFlaubert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.\\n\\nThis model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\\n\\nThis model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\\n\\nforward\\n\\n< source\\n\\n( inputids: typing.Optionaltorch.Tensor = Noneattentionmask: typing.Optionaltorch.Tensor = Nonelangs: typing.Optionaltorch.Tensor = Nonetokentypeids: typing.Optionaltorch.Tensor = Nonepositionids: typing.Optionaltorch.Tensor = Nonelengths: typing.Optionaltorch.Tensor = Nonecache: typing.Uniontyping.Dictstr, torch.Tensor, NoneType = Noneheadmask: typing.Optionaltorch.Tensor = Noneinputsembeds: typing.Optionaltorch.Tensor = Nonelabels: typing.Optionaltorch.Tensor = Noneoutputattentions: typing.Optionalbool = Noneoutputhiddenstates: typing.Optionalbool = Nonereturndict: typing.Optionalbool = None ) → transformers.modelingoutputs.MultipleChoiceModelOutput or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nThe FlaubertForMultipleChoice forward method, overrides the\\n\\n```\\n\\\\_\\\\_call\\\\_\\\\_\\n```\\n\\nspecial method.\\n\\nAlthough the recipe for forward pass needs to be defined within this function, one should call the\\n\\n```\\nModule\\n```\\n\\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\\n\\nExample:\\n\\n```\\n>>> from transformers import AutoTokenizer, FlaubertForMultipleChoice\\n>>> import torch\\n\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"flaubert/flaubert\\\\_base\\\\_cased\")\\n>>> model = FlaubertForMultipleChoice.from\\\\_pretrained(\"flaubert/flaubert\\\\_base\\\\_cased\")\\n\\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\\n>>> choice0 = \"It is eaten with a fork and a knife.\"\\n>>> choice1 = \"It is eaten while held in the hand.\"\\n>>> labels', document_id='165', token_count=512)\n",
      "0.6455808621466147\n",
      "Chunk(content=\" None outputhiddenstates: typing.Optionalbool = None returndict: typing.Optionalbool = None train: bool = False params: dict = None pastkeyvalues: dict = None dropoutrng: PRNGKey = None ) → transformers.modelingflaxoutputs.FlaxBaseModelOutputWithPastAndCrossAttentions or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nParameters\\n\\n [<RawText children='input'>, <Literal children='\\\\_'>, <RawText children='ids'>]\\n\\n [<RawText children='attention'>, <Literal children='\\\\_'>, <RawText children='mask'>]\\n\\n [<RawText children='position'>, <Literal children='\\\\_'>, <RawText children='ids'>]\\n\\n [<RawText children='output'>, <Literal children='\\\\_'>, <RawText children='attentions'>]\\n\\n [<RawText children='output'>, <Literal children='\\\\_'>, <RawText children='hidden'>, <Literal children='\\\\_'>, <RawText children='states'>]\\n\\n [<RawText children='return'>, <Literal children='\\\\_'>, <RawText children='dict'>]\\n\\nA transformers.modelingflaxoutputs.FlaxBaseModelOutputWithPastAndCrossAttentions or a tuple of\\n\\n```\\ntorch.FloatTensor\\n```\\n\\n(if\\n\\n```\\nreturn\\\\_dict=False\\n```\\n\\nis passed or when\\n\\n```\\nconfig.return\\\\_dict=False\\n```\\n\\n) comprising various elements depending on the configuration (XGLMConfig) and inputs.\\n\\n [<RawText children='last'>, <Literal children='\\\\_'>, <RawText children='hidden'>, <Literal children='\\\\_'>, <RawText children='state'>]\\n\\n [<RawText children='past'>, <Literal children='\\\\_'>, <RawText children='key'>, <Literal children='\\\\_'>, <RawText children='values'>]\\n\\n [<RawText children='hidden'>, <Literal children='\\\\_'>, <RawText children='states'>]\\n\\n [<RawText children='attentions'>]\\n\\n [<RawText children='cross'>, <Literal children='\\\\_'>, <RawText children='attentions'>]\\n\\nThe\\n\\n```\\nFlaxXGLMPreTrainedModel\\n```\\n\\nforward method, overrides the\\n\\n```\\n\\\\_\\\\_call\\\\_\\\\_\\n```\\n\\nspecial method.\\n\\nAlthough the recipe for forward pass needs to be defined within this function, one should call the\\n\\n```\\nModule\\n```\\n\\ninstance afterwards instead of this since the former takes care of running the pre\", document_id='309', token_count=512)\n",
      "0.6438103442109543\n",
      "Chunk(content=' <Literal children=\\'\\\\_\\'>, <RawText children=\\'attentions\\'>]\\n\\n [<RawText children=\\'output\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'states\\'>]\\n\\n [<RawText children=\\'return\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'dict\\'>]\\n\\nA transformers.modelingflaxoutputs.FlaxBaseModelOutputWithPastAndCrossAttentions or a tuple of\\n\\n```\\ntorch.FloatTensor\\n```\\n\\n(if\\n\\n```\\nreturn\\\\_dict=False\\n```\\n\\nis passed or when\\n\\n```\\nconfig.return\\\\_dict=False\\n```\\n\\n) comprising various elements depending on the configuration (GPT2Config) and inputs.\\n\\n [<RawText children=\\'last\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'state\\'>]\\n\\n [<RawText children=\\'past\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'key\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'values\\'>]\\n\\n [<RawText children=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'states\\'>]\\n\\n [<RawText children=\\'attentions\\'>]\\n\\n [<RawText children=\\'cross\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'attentions\\'>]\\n\\nThe\\n\\n```\\nFlaxGPT2PreTrainedModel\\n```\\n\\nforward method, overrides the\\n\\n```\\n\\\\_\\\\_call\\\\_\\\\_\\n```\\n\\nspecial method.\\n\\nAlthough the recipe for forward pass needs to be defined within this function, one should call the\\n\\n```\\nModule\\n```\\n\\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\\n\\nExample:\\n\\n```\\n>>> from transformers import AutoTokenizer, FlaxGPT2Model\\n\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"gpt2\")\\n>>> model = FlaxGPT2Model.from\\\\_pretrained(\"gpt2\")\\n\\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return\\\\_tensors=\"jax\")\\n>>> outputs = model(**inputs)\\n\\n>>> last\\\\_hidden\\\\_states = outputs.last\\\\_hidden\\\\_state\\n```\\n\\nFlaxGPT2LMHeadModel\\n\\nclass transformers.FlaxGPT2LMHeadModel\\n\\n< source\\n\\n( config: GPT2Config inputshape: typing.Tuple = (1, 1) seed: int = 0 dtype: dtype', document_id='173', token_count=512)\n",
      "0.6374187536159951\n",
      "Chunk(content='cased\")\\n>>> model = FlaubertModel.from\\\\_pretrained(\"flaubert/flaubert\\\\_base\\\\_cased\")\\n\\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return\\\\_tensors=\"pt\")\\n>>> outputs = model(**inputs)\\n\\n>>> last\\\\_hidden\\\\_states = outputs.last\\\\_hidden\\\\_state\\n```\\n\\nFlaubertWithLMHeadModel\\n\\nclass transformers.FlaubertWithLMHeadModel\\n\\n< source\\n\\n( config )\\n\\nParameters\\n\\n [<RawText children=\\'config\\'>]\\n\\nThe Flaubert Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\\n\\nThis model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\\n\\nThis model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\\n\\nforward\\n\\n< source\\n\\n( inputids: typing.Optionaltorch.Tensor = Noneattentionmask: typing.Optionaltorch.Tensor = Nonelangs: typing.Optionaltorch.Tensor = Nonetokentypeids: typing.Optionaltorch.Tensor = Nonepositionids: typing.Optionaltorch.Tensor = Nonelengths: typing.Optionaltorch.Tensor = Nonecache: typing.Uniontyping.Dictstr, torch.Tensor, NoneType = Noneheadmask: typing.Optionaltorch.Tensor = Noneinputsembeds: typing.Optionaltorch.Tensor = Nonelabels: typing.Optionaltorch.Tensor = Noneoutputattentions: typing.Optionalbool = Noneoutputhiddenstates: typing.Optionalbool = Nonereturndict: typing.Optionalbool = None ) → transformers.modelingoutputs.MaskedLMOutput or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nThe FlaubertWithLMHeadModel forward method, overrides the\\n\\n```\\n\\\\_\\\\_call\\\\_\\\\_\\n```\\n\\nspecial method.\\n\\nAlthough the recipe for forward pass needs to be defined within this function, one should call the\\n\\n```\\nModule\\n```\\n\\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\\n\\nExample:\\n\\n```\\n>>> from transformers import AutoTokenizer, FlaubertWithLMHeadModel\\n>>> import torch\\n\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"flaubert/flaubert\\\\_base\\\\_cased\")\\n>>> model = FlaubertWithLMHeadModel.from\\\\_pretrained(\"flaub', document_id='165', token_count=512)\n",
      "0.6366055990519301\n",
      "Chunk(content='=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'states\\'>]\\n\\n [<RawText children=\\'return\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'dict\\'>]\\n\\nA transformers.modelingflaxoutputs.FlaxBaseModelOutput or a tuple of\\n\\n```\\ntorch.FloatTensor\\n```\\n\\n(if\\n\\n```\\nreturn\\\\_dict=False\\n```\\n\\nis passed or when\\n\\n```\\nconfig.return\\\\_dict=False\\n```\\n\\n) comprising various elements depending on the configuration (\\n\\n```\\n<class \\'transformers.models.longt5.configuration\\\\_longt5.LongT5Config\\'>\\n```\\n\\n) and inputs.\\n\\n [<RawText children=\\'last\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'state\\'>]\\n\\n [<RawText children=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'states\\'>]\\n\\n [<RawText children=\\'attentions\\'>]\\n\\nExample:\\n\\n```\\n>>> from transformers import AutoTokenizer, FlaxLongT5ForConditionalGeneration\\n\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"t5-base\")\\n>>> model = FlaxLongT5ForConditionalGeneration.from\\\\_pretrained(\"google/long-t5-local-base\")\\n\\n>>> text = \"My friends are cool but they eat too many carbs.\"\\n>>> inputs = tokenizer(text, return\\\\_tensors=\"np\")\\n>>> encoder\\\\_outputs = model.encode(**inputs)\\n```\\n\\ndecode\\n\\n< source\\n\\n( decoderinputids encoderoutputs encoderattentionmask: typing.Optionaljax.Array = None decoderattentionmask: typing.Optionaljax.Array = None pastkeyvalues: dict = None outputattentions: typing.Optionalbool = None outputhiddenstates: typing.Optionalbool = None returndict: typing.Optionalbool = None train: bool = False params: dict = None dropoutrng: PRNGKey = None ) → transformers.modelingflaxoutputs.FlaxBaseModelOutputWithPastAndCrossAttentions or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nParameters\\n\\n [<RawText children=\\'decoder\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'input\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'ids\\'>]\\n\\n [<RawText children=\\'encoder\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'outputs\\'>]\\n\\n [<RawText children=\\'encoder\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'attention\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'mask\\'>]\\n\\n [<', document_id='198', token_count=512)\n",
      "0.6356340892193422\n",
      "Chunk(content='\\\\_ids[0]]\\n\\n>>> labels = predicted\\\\_token\\\\_class\\\\_ids\\n>>> loss = model(**inputs, labels=labels).loss\\n```\\n\\nFlaubertForQuestionAnsweringSimple\\n\\nclass transformers.FlaubertForQuestionAnsweringSimple\\n\\n< source\\n\\n( config )\\n\\nParameters\\n\\n [<RawText children=\\'config\\'>]\\n\\nFlaubert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of the hidden-states output to compute\\n\\n```\\nspan start logits\\n```\\n\\nand\\n\\n```\\nspan end logits\\n```\\n\\n).\\n\\nThis model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\\n\\nThis model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\\n\\nforward\\n\\n< source\\n\\n( inputids: typing.Optionaltorch.Tensor = Noneattentionmask: typing.Optionaltorch.Tensor = Nonelangs: typing.Optionaltorch.Tensor = Nonetokentypeids: typing.Optionaltorch.Tensor = Nonepositionids: typing.Optionaltorch.Tensor = Nonelengths: typing.Optionaltorch.Tensor = Nonecache: typing.Uniontyping.Dictstr, torch.Tensor, NoneType = Noneheadmask: typing.Optionaltorch.Tensor = Noneinputsembeds: typing.Optionaltorch.Tensor = Nonestartpositions: typing.Optionaltorch.Tensor = Noneendpositions: typing.Optionaltorch.Tensor = Noneoutputattentions: typing.Optionalbool = Noneoutputhiddenstates: typing.Optionalbool = Nonereturndict: typing.Optionalbool = None ) → transformers.modelingoutputs.QuestionAnsweringModelOutput or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nThe FlaubertForQuestionAnsweringSimple forward method, overrides the\\n\\n```\\n\\\\_\\\\_call\\\\_\\\\_\\n```\\n\\nspecial method.\\n\\nAlthough the recipe for forward pass needs to be defined within this function, one should call the\\n\\n```\\nModule\\n```\\n\\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\\n\\nExample:\\n\\n```\\n>>> from transformers import AutoTokenizer, FlaubertForQuestionAnsweringSimple\\n>>> import torch\\n\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"flaubert/flaubert\\\\_base\\\\_cased\")\\n>>> model = Flaub', document_id='165', token_count=512)\n",
      "0.6318881208219916\n",
      "Chunk(content='trained(\"junnyu/roformer\\\\_chinese\\\\_base\")\\n>>> model = FlaxRoFormerForMaskedLM.from\\\\_pretrained(\"junnyu/roformer\\\\_chinese\\\\_base\")\\n\\n>>> inputs = tokenizer(\"The capital of France is [MASK].\", return\\\\_tensors=\"jax\")\\n\\n>>> outputs = model(**inputs)\\n>>> logits = outputs.logits\\n```\\n\\nFlaxRoFormerForSequenceClassification\\n\\nclass transformers.FlaxRoFormerForSequenceClassification\\n\\n< source\\n\\n( config: RoFormerConfig inputshape: typing.Tuple = (1, 1) seed: int = 0 dtype: dtype = <class \\'jax.numpy.float32\\'> doinit: bool = True kwargs )\\n\\nParameters\\n\\n [<RawText children=\\'config\\'>]\\n\\n [<RawText children=\\'dtype\\'>]\\n\\nRoFormer Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.\\n\\nThis model inherits from FlaxPreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading, saving and converting weights from PyTorch models)\\n\\nThis model is also a Flax Linen flax.linen.Module subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and behavior.\\n\\nFinally, this model supports inherent JAX features such as:\\n\\n [<RawText children=\\'Just-In-Time (JIT) compilation\\'>]\\n\\n [<RawText children=\\'Automatic Differentiation\\'>]\\n\\n [<RawText children=\\'Vectorization\\'>]\\n\\n [<RawText children=\\'Parallelization\\'>]\\n\\n\\\\_\\\\_call\\\\_\\\\_\\n\\n< source\\n\\n( inputids attentionmask = None tokentypeids = None headmask = None params: dict = None dropoutrng: PRNGKey = None train: bool = False outputattentions: typing.Optionalbool = None outputhiddenstates: typing.Optionalbool = None returndict: typing.Optionalbool = None ) → transformers.modelingflaxoutputs.FlaxSequenceClassifierOutput or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nParameters\\n\\n [<RawText children=\\'input\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'ids\\'>]\\n\\n [<RawText children=\\'attention\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'mask\\'>]\\n\\n [<RawText children=\\'token\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'type\\'>, <Literal children=\\'\\\\_\\'>, <RawText', document_id='256', token_count=512)\n",
      "0.6313718645472836\n",
      "Chunk(content=' or loaded from\\n\\n```\\npretrained\\\\_model\\\\_name\\\\_or\\\\_path\\n```\\n\\nif possible), or when it’s missing, by falling back to using pattern matching on\\n\\n```\\npretrained\\\\_model\\\\_name\\\\_or\\\\_path\\n```\\n\\n:\\n\\n [<RawText children=\\'beit\\'>]\\n\\n [<RawText children=\\'regnet\\'>]\\n\\n [<RawText children=\\'resnet\\'>]\\n\\n [<RawText children=\\'vit\\'>]\\n\\nExamples:\\n\\n```\\n>>> from transformers import AutoConfig, FlaxAutoModelForImageClassification\\n\\n>>> \\n>>> model = FlaxAutoModelForImageClassification.from\\\\_pretrained(\"bert-base-cased\")\\n\\n>>> \\n>>> model = FlaxAutoModelForImageClassification.from\\\\_pretrained(\"bert-base-cased\", output\\\\_attentions=True)\\n>>> model.config.output\\\\_attentions\\nTrue\\n\\n>>> \\n>>> config = AutoConfig.from\\\\_pretrained(\"./pt\\\\_model/bert\\\\_pt\\\\_model\\\\_config.json\")\\n>>> model = FlaxAutoModelForImageClassification.from\\\\_pretrained(\\n...     \"./pt\\\\_model/bert\\\\_pytorch\\\\_model.bin\", from\\\\_pt=True, config=config\\n... )\\n```\\n\\nAutoModelForVideoClassification\\n\\nclass transformers.AutoModelForVideoClassification\\n\\n< source\\n\\n( argskwargs )\\n\\nThis is a generic model class that will be instantiated as one of the model classes of the library (with a video classification head) when created with the frompretrained() class method or the fromconfig() class method.\\n\\nThis class cannot be instantiated directly using\\n\\n```\\n\\\\_\\\\_init\\\\_\\\\_()\\n```\\n\\n(throws an error).\\n\\nfrom\\\\_config\\n\\n< source\\n\\n( kwargs )\\n\\nParameters\\n\\n [<RawText children=\\'config\\'>]\\n\\nInstantiates one of the model classes of the library (with a video classification head) from a configuration.\\n\\nNote: Loading a model from its configuration file does not load the model weights. It only affects the model’s configuration. Use frompretrained() to load the model weights.\\n\\nExamples:\\n\\n```\\n>>> from transformers import AutoConfig, AutoModelForVideoClassification\\n\\n>>> \\n>>> config = AutoConfig.from\\\\_pretrained(\"bert-base-cased\")\\n>>> model = AutoModelForVideoClassification.from\\\\_config(config)\\n```\\n\\nInstantiate one of the model classes of the library (with a video classification head) from a pretrained model.\\n\\nThe model class to instantiate is selected based on the\\n\\n```\\nmodel\\\\_type\\n```\\n\\nproperty of the config object (either passed as an argument or loaded from\\n\\n```\\npretrained\\\\_model\\\\_name\\\\_or', document_id='70', token_count=512)\n",
      "0.6303673622623528\n",
      "Chunk(content=' attentionmask = None tokentypeids = None headmask = None params: dict = None dropoutrng: PRNGKey = None train: bool = False outputattentions: typing.Optionalbool = None outputhiddenstates: typing.Optionalbool = None returndict: typing.Optionalbool = None ) → transformers.modelingflaxoutputs.FlaxMultipleChoiceModelOutput or\\n\\n```\\ntuple(torch.FloatTensor)\\n```\\n\\nParameters\\n\\n [<RawText children=\\'input\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'ids\\'>]\\n\\n [<RawText children=\\'attention\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'mask\\'>]\\n\\n [<RawText children=\\'token\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'type\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'ids\\'>]\\n\\n [<RawText children=\\'position\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'ids\\'>]\\n\\n [<RawText children=\\'head\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'mask\\'>]\\n\\n [<RawText children=\\'return\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'dict\\'>]\\n\\nA transformers.modelingflaxoutputs.FlaxMultipleChoiceModelOutput or a tuple of\\n\\n```\\ntorch.FloatTensor\\n```\\n\\n(if\\n\\n```\\nreturn\\\\_dict=False\\n```\\n\\nis passed or when\\n\\n```\\nconfig.return\\\\_dict=False\\n```\\n\\n) comprising various elements depending on the configuration (RoFormerConfig) and inputs.\\n\\n [<RawText children=\\'logits\\'>]\\n\\n [<RawText children=\\'hidden\\'>, <Literal children=\\'\\\\_\\'>, <RawText children=\\'states\\'>]\\n\\n [<RawText children=\\'attentions\\'>]\\n\\nThe\\n\\n```\\nFlaxRoFormerPreTrainedModel\\n```\\n\\nforward method, overrides the\\n\\n```\\n\\\\_\\\\_call\\\\_\\\\_\\n```\\n\\nspecial method.\\n\\nAlthough the recipe for forward pass needs to be defined within this function, one should call the\\n\\n```\\nModule\\n```\\n\\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\\n\\nExample:\\n\\n```\\n>>> from transformers import AutoTokenizer, FlaxRoFormerForMultipleChoice\\n\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"junnyu/roformer\\\\_chinese\\\\_base\")\\n>>> model = FlaxRoFormerForMultipleChoice.from\\\\_pretrained(\"junnyu/roformer\\\\_chinese\\\\_base\")\\n\\n>>> prompt = \"In Italy, pizza served in', document_id='256', token_count=512)\n",
      "0.6253748398655467\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Test run\n",
    "client = LlamaStackClient(\n",
    "    base_url=\"http://localhost:5001\",\n",
    ")\n",
    "\n",
    "response = client.memory.query(\n",
    "    bank_id=bank_id,\n",
    "    query=[\"What is William Falder's occupation?\"],\n",
    "    params={\"max_chunks\": 10},\n",
    ")\n",
    "\n",
    "display(response)\n",
    "\n",
    "for chunk, score in zip(response.chunks, response.scores):\n",
    "    print(chunk)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>386.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30982.411917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>34134.173003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>129.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8419.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20803.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39813.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>213379.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       context_length\n",
       "count      386.000000\n",
       "mean     30982.411917\n",
       "std      34134.173003\n",
       "min        129.000000\n",
       "25%       8419.000000\n",
       "50%      20803.000000\n",
       "75%      39813.750000\n",
       "max     213379.000000"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"philschmid/markdown-documentation-transformers\")\n",
    "\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "df = df.rename(columns={'markdown': 'context'})\n",
    "df['context_length'] = df['context'].apply(lambda x: len(x))\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386\n",
      "386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['🤗 Transformers',\n",
       " 'Quick tour',\n",
       " 'Installation',\n",
       " 'Pipelines for inference',\n",
       " 'Load pretrained instances with an AutoClass']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_ids = []\n",
    "titles = []\n",
    "for i, row in df.iterrows():\n",
    "    titles.append(row[\"title\"])\n",
    "    doc_ids.append(i)\n",
    "\n",
    "print(len(titles))\n",
    "print(len(doc_ids))\n",
    "display(titles[:5])\n",
    "doc_ids[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▋                                                                      | 19/386 [00:00<00:04, 91.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 386/386 [00:20<00:00, 19.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[46, 23, 1, 62, 54, 45, 47, 52, 47, 24],\n",
       " [80, 175, 32, 34, 18, 338, 0, 316, 168, 44],\n",
       " [168, 316, 167, 80, 47, 83, 2, 0, 237, 264],\n",
       " [3, 84, 77, 59, 1, 77, 48, 77, 77, 77],\n",
       " [4, 70, 70, 70, 70, 70, 70, 70, 70, 70],\n",
       " [287, 195, 312, 312, 147, 257, 255, 48, 255, 329],\n",
       " [15, 15, 1, 71, 336, 326, 210, 336, 354, 333],\n",
       " [1, 80, 64, 7, 375, 83, 384, 64, 83, 329],\n",
       " [8, 8, 28, 384, 7, 31, 83, 29, 29, 34],\n",
       " [9, 9, 9, 9, 46, 75, 75, 24, 240, 70],\n",
       " [10, 47, 10, 17, 47, 47, 336, 15, 333, 15],\n",
       " [11, 24, 23, 63, 62, 1, 11, 54, 24, 46],\n",
       " [12, 12, 12, 194, 194, 194, 194, 224, 193, 193],\n",
       " [13, 13, 253, 134, 192, 131, 47, 78, 217, 110],\n",
       " [14, 286, 215, 120, 308, 360, 157, 315, 310, 312],\n",
       " [15, 17, 251, 46, 15, 70, 17, 46, 325, 47],\n",
       " [16, 16, 16, 16, 85, 16, 16, 77, 16, 110],\n",
       " [10, 47, 17, 10, 15, 47, 47, 333, 47, 17],\n",
       " [384, 83, 83, 7, 36, 18, 360, 338, 28, 326],\n",
       " [19, 19, 20, 179, 166, 260, 124, 154, 183, 260],\n",
       " [20, 20, 72, 20, 72, 72, 20, 20, 20, 72],\n",
       " [21, 228, 350, 173, 256, 198, 118, 232, 120, 106],\n",
       " [22, 22, 22, 22, 30, 22, 74, 115, 22, 28],\n",
       " [23, 46, 1, 47, 45, 276, 47, 47, 197, 7],\n",
       " [24, 24, 11, 305, 24, 24, 24, 24, 24, 11],\n",
       " [10, 316, 45, 109, 343, 375, 114, 168, 179, 144],\n",
       " [45, 168, 26, 42, 47, 42, 80, 304, 46, 208],\n",
       " [80, 30, 29, 22, 29, 71, 28, 59, 27, 74],\n",
       " [29, 28, 27, 34, 40, 60, 35, 38, 83, 28],\n",
       " [29, 28, 83, 27, 40, 29, 35, 83, 28, 60],\n",
       " [28, 29, 30, 38, 28, 28, 30, 34, 28, 60],\n",
       " [29, 31, 8, 28, 28, 30, 38, 28, 27, 83],\n",
       " [32, 33, 33, 33, 80, 273, 275, 275, 83, 188],\n",
       " [33, 7, 188, 226, 133, 226, 271, 173, 132, 120],\n",
       " [34, 83, 35, 27, 29, 28, 83, 38, 8, 60],\n",
       " [83, 35, 34, 15, 83, 15, 8, 27, 29, 337],\n",
       " [36, 83, 36, 64, 329, 326, 83, 83, 340, 336],\n",
       " [39, 37, 88, 27, 77, 38, 249, 182, 28, 60],\n",
       " [29, 40, 28, 27, 60, 29, 60, 38, 80, 28],\n",
       " [39, 38, 34, 60, 40, 37, 30, 29, 27, 249],\n",
       " [40, 29, 27, 60, 83, 28, 83, 29, 22, 28],\n",
       " [75, 15, 75, 47, 41, 47, 47, 10, 38, 12],\n",
       " [42, 68, 140, 64, 42, 50, 47, 31, 88, 42],\n",
       " [43, 43, 33, 43, 33, 27, 310, 317, 312, 310],\n",
       " [44, 44, 37, 21, 169, 91, 28, 347, 217, 126],\n",
       " [45, 46, 45, 47, 1, 23, 47, 47, 47, 47],\n",
       " [46, 47, 46, 20, 132, 309, 148, 336, 151, 340],\n",
       " [47, 45, 46, 47, 47, 17, 47, 1, 47, 47],\n",
       " [48, 48, 3, 48, 77, 84, 84, 77, 77, 77],\n",
       " [45, 49, 46, 47, 49, 45, 342, 50, 47, 49],\n",
       " [50, 50, 50, 50, 50, 50, 50, 50, 50, 46],\n",
       " [51, 375, 168, 316, 185, 176, 375, 362, 18, 362],\n",
       " [46, 1, 54, 52, 23, 47, 62, 45, 47, 24],\n",
       " [345, 375, 375, 375, 53, 52, 255, 56, 3, 197],\n",
       " [54, 47, 1, 23, 46, 52, 62, 47, 45, 257],\n",
       " [55, 78, 55, 5, 173, 66, 85, 189, 218, 55],\n",
       " [56, 78, 78, 229, 253, 352, 131, 164, 352, 192],\n",
       " [144, 200, 178, 57, 192, 237, 290, 162, 62, 161],\n",
       " [58, 22, 104, 104, 147, 104, 208, 1, 104, 136],\n",
       " [59, 3, 48, 84, 77, 3, 77, 77, 48, 77],\n",
       " [15, 34, 32, 189, 384, 333, 337, 1, 343, 15],\n",
       " [61, 61, 71, 282, 134, 53, 120, 203, 5, 317],\n",
       " [62, 47, 46, 183, 17, 45, 63, 23, 54, 195],\n",
       " [24, 63, 24, 11, 11, 24, 24, 24, 24, 11],\n",
       " [64, 64, 67, 320, 217, 221, 120, 232, 243, 120],\n",
       " [319, 349, 17, 262, 127, 272, 191, 179, 226, 163],\n",
       " [66, 335, 66, 255, 362, 66, 75, 152, 185, 360],\n",
       " [67, 342, 64, 64, 329, 161, 134, 343, 363, 254],\n",
       " [68, 68, 68, 83, 64, 91, 184, 256, 83, 106],\n",
       " [12, 69, 325, 12, 142, 77, 368, 91, 107, 325],\n",
       " [96, 70, 15, 70, 70, 70, 70, 96, 17, 71],\n",
       " [47, 47, 47, 15, 15, 325, 0, 217, 185, 47],\n",
       " [72, 20, 20, 20, 20, 72, 20, 72, 46, 17],\n",
       " [73, 73, 73, 73, 91, 288, 73, 205, 36, 91],\n",
       " [74, 39, 30, 31, 30, 88, 283, 29, 28, 60],\n",
       " [75, 47, 46, 17, 45, 47, 75, 47, 75, 1],\n",
       " [184, 221, 219, 76, 147, 76, 124, 76, 310, 91],\n",
       " [48, 3, 77, 84, 59, 77, 77, 29, 48, 48],\n",
       " [253, 164, 127, 192, 229, 78, 131, 78, 56, 309],\n",
       " [79, 153, 204, 15, 145, 199, 292, 288, 5, 15],\n",
       " [80, 80, 80, 80, 80, 80, 80, 358, 80, 80],\n",
       " [87, 81, 292, 113, 237, 283, 111, 244, 126, 285],\n",
       " [82, 15, 71, 93, 48, 17, 113, 216, 219, 293],\n",
       " [64, 64, 83, 83, 83, 384, 17, 1, 83, 83],\n",
       " [48, 3, 84, 77, 59, 77, 77, 77, 48, 84],\n",
       " [253, 309, 78, 78, 131, 164, 210, 127, 229, 352],\n",
       " [79, 86, 153, 204, 79, 199, 265, 15, 204, 89],\n",
       " [87, 81, 113, 283, 292, 111, 244, 112, 126, 212],\n",
       " [88, 83, 83, 83, 7, 83, 1, 80, 64, 83],\n",
       " [342, 90, 89, 88, 91, 349, 212, 84, 29, 199],\n",
       " [90, 278, 185, 96, 88, 185, 33, 0, 86, 43],\n",
       " [69, 12, 325, 91, 12, 77, 325, 142, 91, 239],\n",
       " [168, 92, 0, 186, 152, 153, 113, 301, 218, 0],\n",
       " [93, 93, 93, 93, 93, 93, 93, 305, 305, 93],\n",
       " [94, 106, 94, 94, 94, 94, 194, 94, 193, 94],\n",
       " [95, 86, 95, 86, 95, 153, 52, 86, 86, 264],\n",
       " [96, 96, 96, 96, 96, 96, 195, 256, 63, 256],\n",
       " [97, 97, 97, 97, 97, 77, 97, 97, 97, 379],\n",
       " [98, 100, 98, 100, 98, 100, 100, 99, 168, 99],\n",
       " [98, 100, 99, 100, 98, 98, 99, 100, 100, 98],\n",
       " [100, 98, 100, 98, 100, 100, 99, 100, 98, 99],\n",
       " [101, 101, 101, 150, 168, 101, 203, 101, 101, 101],\n",
       " [22, 104, 58, 104, 102, 47, 104, 104, 104, 22],\n",
       " [103, 103, 58, 22, 208, 104, 217, 249, 104, 147],\n",
       " [22, 166, 104, 104, 58, 22, 70, 47, 208, 245],\n",
       " [105, 105, 106, 105, 106, 106, 106, 106, 106, 106],\n",
       " [105, 105, 106, 106, 106, 106, 106, 106, 106, 106],\n",
       " [107, 107, 107, 173, 107, 107, 107, 175, 170, 177],\n",
       " [108, 108, 77, 42, 75, 223, 357, 88, 207, 38],\n",
       " [109, 110, 109, 109, 222, 188, 24, 317, 110, 79],\n",
       " [109, 110, 110, 24, 11, 24, 24, 109, 11, 24],\n",
       " [112, 111, 112, 112, 111, 364, 111, 111, 112, 111],\n",
       " [112, 111, 112, 112, 364, 111, 112, 111, 112, 112],\n",
       " [113, 113, 113, 113, 113, 113, 113, 113, 113, 0],\n",
       " [114, 114, 114, 114, 114, 80, 114, 59, 114, 114],\n",
       " [115, 115, 202, 210, 168, 52, 375, 77, 106, 307],\n",
       " [316, 116, 116, 328, 179, 116, 168, 343, 56, 385],\n",
       " [275, 286, 275, 273, 198, 286, 275, 198, 264, 275],\n",
       " [118, 118, 118, 118, 118, 118, 118, 118, 118, 305],\n",
       " [119, 119, 119, 119, 77, 119, 154, 221, 119, 119],\n",
       " [120, 54, 174, 120, 224, 170, 177, 120, 170, 120],\n",
       " [121, 121, 121, 121, 121, 121, 121, 121, 121, 121],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [222, 357, 123, 247, 222, 123, 200, 193, 262, 244],\n",
       " [124, 124, 93, 305, 125, 124, 124, 124, 124, 118],\n",
       " [125, 125, 125, 125, 124, 305, 301, 291, 124, 124],\n",
       " [126, 143, 143, 141, 143, 141, 0, 138, 143, 138],\n",
       " [0, 127, 130, 304, 353, 127, 168, 287, 202, 255],\n",
       " [128, 130, 130, 128, 128, 130, 54, 128, 293, 336],\n",
       " [129, 129, 202, 129, 129, 0, 174, 375, 199, 149],\n",
       " [130, 130, 128, 130, 128, 128, 54, 128, 130, 289],\n",
       " [129, 131, 199, 129, 313, 0, 33, 202, 31, 12],\n",
       " [0, 194, 204, 208, 353, 107, 171, 151, 305, 353],\n",
       " [133, 294, 128, 293, 54, 296, 219, 244, 133, 139],\n",
       " [134, 136, 134, 136, 136, 134, 134, 134, 134, 136],\n",
       " [135, 135, 373, 135, 374, 135, 376, 135, 135, 135],\n",
       " [134, 136, 136, 134, 134, 136, 134, 134, 136, 134],\n",
       " [137, 62, 137, 54, 23, 185, 274, 0, 274, 237],\n",
       " [138, 138, 141, 126, 141, 143, 143, 143, 143, 138],\n",
       " [139, 168, 139, 136, 139, 134, 139, 139, 134, 316],\n",
       " [140, 337, 47, 205, 140, 384, 186, 230, 186, 328],\n",
       " [141, 141, 126, 143, 141, 143, 202, 141, 143, 141],\n",
       " [142, 142, 77, 173, 305, 255, 77, 348, 132, 173],\n",
       " [126, 143, 138, 143, 143, 141, 168, 202, 141, 141],\n",
       " [144, 227, 221, 294, 62, 252, 196, 293, 246, 271],\n",
       " [145, 145, 145, 145, 145, 135, 144, 135, 336, 202],\n",
       " [146, 146, 146, 146, 62, 146, 0, 124, 146, 334],\n",
       " [15, 15, 147, 147, 147, 340, 136, 147, 134, 151],\n",
       " [148, 148, 29, 148, 334, 36, 334, 149, 168, 232],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [170, 168, 139, 186, 202, 181, 326, 150, 175, 202],\n",
       " [151, 96, 195, 195, 256, 151, 232, 151, 195, 195],\n",
       " [152, 152, 251, 226, 151, 216, 152, 254, 152, 226],\n",
       " [351, 153, 153, 186, 42, 113, 351, 119, 91, 42],\n",
       " [154, 154, 154, 202, 316, 154, 154, 0, 375, 154],\n",
       " [351, 155, 155, 351, 62, 143, 315, 53, 354, 155],\n",
       " [157, 156, 157, 156, 157, 156, 157, 157, 77, 0],\n",
       " [157, 157, 156, 157, 157, 156, 157, 156, 156, 156],\n",
       " [158, 284, 112, 307, 160, 198, 71, 106, 256, 202],\n",
       " [159, 159, 159, 159, 77, 159, 159, 106, 159, 38],\n",
       " [160, 158, 112, 307, 158, 173, 223, 165, 147, 165],\n",
       " [161, 161, 161, 161, 161, 161, 161, 161, 202, 168],\n",
       " [162, 162, 254, 162, 152, 162, 162, 254, 124, 162],\n",
       " [163, 163, 163, 163, 163, 163, 163, 163, 307, 154],\n",
       " [152, 164, 251, 164, 164, 254, 254, 164, 164, 164],\n",
       " [165, 165, 165, 165, 165, 165, 165, 165, 147, 165],\n",
       " [166, 166, 166, 166, 166, 166, 166, 62, 166, 166],\n",
       " [167, 167, 45, 167, 175, 45, 83, 46, 10, 361],\n",
       " [168, 168, 168, 171, 28, 177, 168, 173, 61, 349],\n",
       " [169, 173, 169, 169, 82, 349, 177, 175, 171, 169],\n",
       " [170, 171, 171, 170, 170, 170, 171, 170, 170, 172],\n",
       " [170, 171, 171, 171, 170, 170, 170, 170, 170, 171],\n",
       " [172, 178, 178, 170, 172, 174, 0, 172, 171, 178],\n",
       " [173, 169, 169, 173, 175, 169, 82, 177, 349, 170],\n",
       " [174, 173, 170, 173, 173, 175, 177, 173, 61, 173],\n",
       " [175, 175, 170, 175, 173, 61, 173, 177, 173, 175],\n",
       " [176, 176, 176, 176, 0, 337, 337, 205, 0, 203],\n",
       " [61, 177, 170, 173, 177, 173, 174, 177, 172, 173],\n",
       " [178, 178, 172, 178, 169, 0, 178, 178, 172, 173],\n",
       " [179, 179, 179, 179, 77, 263, 304, 172, 144, 172],\n",
       " [180, 316, 0, 258, 113, 94, 0, 312, 94, 328],\n",
       " [328, 328, 0, 328, 328, 181, 181, 65, 375, 154],\n",
       " [22, 166, 245, 104, 1, 70, 22, 104, 58, 182],\n",
       " [183, 183, 316, 168, 156, 157, 129, 363, 186, 215],\n",
       " [184, 184, 244, 351, 184, 184, 244, 44, 133, 237],\n",
       " [185, 185, 185, 375, 185, 62, 311, 185, 0, 232],\n",
       " [186, 186, 186, 186, 186, 186, 186, 186, 186, 77],\n",
       " [190, 187, 187, 188, 187, 187, 187, 189, 190, 187],\n",
       " [188, 190, 187, 188, 187, 190, 187, 189, 188, 187],\n",
       " [190, 188, 187, 189, 187, 189, 190, 187, 187, 187],\n",
       " [190, 190, 188, 187, 187, 188, 188, 187, 187, 189],\n",
       " [191, 192, 253, 334, 191, 127, 100, 191, 182, 109],\n",
       " [192, 192, 192, 192, 192, 192, 120, 228, 200, 200],\n",
       " [194, 357, 194, 194, 235, 193, 193, 194, 194, 194],\n",
       " [194, 194, 357, 235, 194, 194, 193, 193, 357, 194],\n",
       " [195, 195, 192, 195, 195, 195, 195, 195, 195, 195],\n",
       " [196, 196, 196, 196, 196, 196, 196, 177, 168, 173],\n",
       " [197, 197, 197, 197, 197, 167, 173, 184, 174, 224],\n",
       " [198, 198, 198, 198, 198, 198, 195, 273, 273, 195],\n",
       " [199, 199, 202, 53, 199, 132, 129, 132, 375, 199],\n",
       " [200, 200, 200, 200, 200, 200, 200, 200, 200, 200],\n",
       " [201, 201, 201, 201, 201, 357, 14, 207, 235, 194],\n",
       " [168, 202, 343, 202, 202, 202, 202, 202, 0, 202],\n",
       " [203, 203, 203, 203, 203, 203, 203, 203, 203, 101],\n",
       " [204, 0, 204, 375, 230, 18, 113, 312, 204, 78],\n",
       " [205, 205, 114, 168, 194, 202, 343, 279, 77, 217],\n",
       " [206, 206, 209, 206, 206, 206, 209, 209, 209, 233],\n",
       " [207, 207, 207, 207, 208, 207, 193, 207, 207, 168],\n",
       " [208, 208, 208, 208, 208, 207, 208, 46, 134, 208],\n",
       " [206, 209, 209, 206, 209, 353, 233, 297, 209, 202],\n",
       " [210, 210, 210, 210, 210, 210, 210, 210, 210, 210],\n",
       " [208, 207, 208, 208, 170, 208, 29, 174, 194, 211],\n",
       " [212, 212, 212, 212, 212, 212, 199, 174, 175, 169],\n",
       " [213, 213, 213, 213, 213, 213, 224, 200, 29, 246],\n",
       " [168, 214, 375, 357, 224, 193, 0, 214, 199, 348],\n",
       " [215, 215, 228, 64, 235, 36, 156, 0, 118, 180],\n",
       " [216, 216, 216, 219, 218, 218, 267, 152, 216, 218],\n",
       " [217, 217, 217, 134, 217, 269, 217, 217, 217, 217],\n",
       " [216, 216, 218, 218, 216, 219, 152, 218, 218, 267],\n",
       " [219, 219, 219, 217, 267, 219, 219, 197, 219, 217],\n",
       " [219, 220, 216, 219, 219, 220, 218, 219, 44, 267],\n",
       " [221, 221, 221, 221, 221, 221, 168, 240, 221, 0],\n",
       " [222, 222, 222, 222, 222, 243, 222, 243, 222, 222],\n",
       " [223, 223, 223, 223, 223, 286, 273, 223, 275, 223],\n",
       " [224, 224, 61, 244, 173, 226, 171, 174, 175, 226],\n",
       " [225, 225, 225, 225, 225, 0, 328, 47, 121, 0],\n",
       " [226, 226, 226, 226, 226, 224, 226, 226, 226, 251],\n",
       " [227, 144, 290, 221, 294, 293, 196, 62, 271, 138],\n",
       " [228, 228, 228, 228, 228, 228, 228, 228, 228, 228],\n",
       " [229, 231, 229, 231, 231, 229, 231, 229, 203, 259],\n",
       " [230, 211, 235, 193, 357, 194, 208, 175, 185, 175],\n",
       " [231, 231, 229, 231, 231, 229, 186, 379, 212, 91],\n",
       " [232, 232, 232, 232, 232, 232, 232, 232, 232, 252],\n",
       " [233, 233, 233, 233, 232, 233, 96, 233, 195, 195],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [235, 194, 357, 194, 193, 194, 235, 193, 194, 194],\n",
       " [236, 239, 239, 239, 239, 236, 239, 239, 0, 239],\n",
       " [237, 237, 237, 237, 237, 237, 237, 237, 237, 237],\n",
       " [383, 238, 238, 383, 238, 238, 238, 238, 238, 238],\n",
       " [236, 239, 239, 239, 239, 239, 239, 239, 236, 236],\n",
       " [240, 240, 240, 240, 202, 375, 240, 375, 289, 226],\n",
       " [241, 168, 0, 0, 0, 375, 0, 316, 114, 202],\n",
       " [242, 242, 140, 242, 242, 242, 242, 242, 242, 205],\n",
       " [367, 243, 243, 243, 202, 384, 142, 243, 243, 252],\n",
       " [244, 133, 244, 294, 293, 296, 294, 62, 291, 151],\n",
       " [245, 245, 245, 245, 245, 311, 311, 245, 311, 245],\n",
       " [246, 246, 246, 92, 246, 152, 246, 308, 92, 119],\n",
       " [247, 247, 247, 247, 247, 168, 65, 185, 186, 232],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [370, 249, 249, 249, 249, 249, 249, 249, 191, 249],\n",
       " [250, 250, 250, 0, 250, 250, 193, 194, 56, 114],\n",
       " [251, 251, 254, 254, 251, 226, 152, 251, 164, 124],\n",
       " [252, 252, 252, 252, 252, 57, 252, 252, 252, 197],\n",
       " [253, 253, 352, 253, 46, 0, 253, 128, 253, 83],\n",
       " [254, 254, 152, 251, 251, 17, 17, 124, 254, 254],\n",
       " [255, 255, 255, 134, 255, 255, 255, 255, 134, 269],\n",
       " [256, 256, 256, 256, 256, 256, 256, 256, 256, 256],\n",
       " [257, 257, 257, 257, 257, 257, 257, 257, 258, 257],\n",
       " [258, 257, 257, 257, 257, 258, 257, 257, 257, 257],\n",
       " [259, 259, 259, 259, 259, 259, 0, 237, 168, 202],\n",
       " [260, 260, 260, 260, 260, 260, 260, 260, 202, 263],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [262, 262, 262, 168, 257, 100, 106, 279, 105, 94],\n",
       " [263, 353, 0, 263, 0, 193, 113, 153, 52, 85],\n",
       " [264, 264, 264, 264, 264, 264, 264, 264, 288, 379],\n",
       " [266, 265, 265, 265, 266, 265, 354, 379, 266, 266],\n",
       " [266, 265, 265, 265, 266, 265, 266, 266, 354, 266],\n",
       " [267, 267, 267, 267, 267, 151, 232, 96, 195, 195],\n",
       " [268, 268, 202, 176, 328, 168, 375, 113, 328, 0],\n",
       " [269, 269, 269, 269, 269, 269, 269, 269, 134, 269],\n",
       " [270, 271, 272, 270, 271, 272, 272, 62, 271, 271],\n",
       " [271, 270, 271, 272, 271, 62, 270, 272, 272, 271],\n",
       " [270, 272, 272, 271, 272, 270, 271, 162, 271, 270],\n",
       " [273, 273, 275, 275, 275, 275, 223, 275, 275, 275],\n",
       " [274, 274, 274, 274, 274, 274, 0, 274, 256, 232],\n",
       " [273, 275, 275, 273, 275, 275, 275, 275, 275, 275],\n",
       " [276, 276, 276, 62, 23, 52, 54, 77, 0, 1],\n",
       " [277, 277, 0, 337, 277, 0, 0, 202, 121, 237],\n",
       " [185, 62, 96, 278, 90, 54, 1, 46, 23, 259],\n",
       " [279, 279, 279, 279, 279, 279, 279, 279, 279, 25],\n",
       " [278, 0, 280, 90, 96, 185, 96, 96, 73, 185],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [62, 317, 23, 310, 310, 310, 282, 312, 185, 54],\n",
       " [283, 283, 283, 283, 202, 0, 353, 31, 3, 199],\n",
       " [284, 158, 194, 135, 200, 171, 106, 255, 338, 106],\n",
       " [285, 285, 202, 285, 285, 301, 0, 216, 186, 152],\n",
       " [186, 375, 185, 285, 265, 375, 186, 279, 375, 24],\n",
       " [287, 287, 288, 288, 56, 287, 288, 288, 288, 288],\n",
       " [288, 288, 287, 259, 287, 36, 288, 69, 288, 83],\n",
       " [202, 375, 168, 289, 316, 252, 52, 77, 77, 282],\n",
       " [290, 290, 290, 202, 77, 168, 379, 290, 98, 379],\n",
       " [291, 291, 377, 291, 301, 301, 377, 291, 377, 291],\n",
       " [301, 301, 296, 301, 298, 292, 300, 292, 298, 292],\n",
       " [294, 133, 54, 293, 62, 296, 293, 128, 296, 298],\n",
       " [294, 133, 54, 294, 151, 62, 296, 293, 293, 293],\n",
       " [295, 62, 134, 295, 113, 295, 136, 295, 295, 352],\n",
       " [54, 296, 301, 296, 300, 296, 294, 300, 378, 301],\n",
       " [344, 298, 297, 298, 298, 300, 297, 301, 299, 300],\n",
       " [298, 344, 298, 298, 297, 301, 300, 300, 299, 301],\n",
       " [299, 298, 301, 296, 297, 293, 301, 214, 300, 300],\n",
       " [296, 54, 301, 300, 301, 297, 301, 293, 293, 296],\n",
       " [133, 294, 301, 54, 298, 293, 293, 296, 62, 128],\n",
       " [304, 304, 302, 302, 302, 304, 303, 54, 302, 302],\n",
       " [303, 302, 302, 304, 54, 304, 302, 304, 266, 304],\n",
       " [304, 304, 302, 304, 303, 304, 302, 15, 302, 304],\n",
       " [305, 93, 124, 305, 124, 118, 124, 124, 125, 305],\n",
       " [306, 306, 306, 306, 304, 304, 0, 302, 314, 304],\n",
       " [307, 307, 307, 3, 307, 62, 307, 91, 307, 379],\n",
       " [305, 308, 308, 308, 308, 308, 305, 305, 113, 328],\n",
       " [309, 309, 309, 309, 309, 305, 309, 312, 309, 309],\n",
       " [313, 312, 315, 310, 311, 310, 310, 314, 33, 43],\n",
       " [311, 311, 311, 311, 311, 311, 245, 245, 245, 245],\n",
       " [312, 312, 257, 312, 312, 312, 309, 309, 70, 257],\n",
       " [313, 310, 310, 311, 314, 310, 312, 43, 315, 33],\n",
       " [33, 314, 312, 180, 313, 43, 312, 312, 311, 43],\n",
       " [312, 312, 312, 309, 70, 257, 257, 312, 309, 312],\n",
       " [314, 304, 314, 302, 304, 304, 303, 302, 302, 0],\n",
       " [317, 317, 317, 311, 311, 317, 311, 313, 314, 311],\n",
       " [319, 318, 318, 318, 319, 318, 318, 168, 112, 111],\n",
       " [319, 319, 318, 318, 318, 318, 319, 296, 77, 319],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [325, 12, 54, 69, 77, 132, 225, 142, 54, 12],\n",
       " [326, 310, 132, 326, 174, 82, 131, 98, 169, 245],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [328, 328, 328, 337, 113, 328, 181, 237, 233, 238],\n",
       " [329, 25, 375, 54, 375, 52, 25, 375, 275, 25],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [52, 333, 54, 306, 222, 95, 304, 5, 288, 95],\n",
       " [149, 334, 334, 36, 29, 148, 334, 334, 134, 353],\n",
       " [335, 214, 354, 288, 304, 288, 265, 302, 379, 302],\n",
       " [336, 337, 328, 52, 377, 301, 52, 337, 184, 54],\n",
       " [337, 54, 52, 296, 52, 143, 238, 138, 238, 319],\n",
       " [338, 52, 209, 233, 260, 219, 179, 218, 54, 233],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [340, 53, 56, 52, 275, 229, 189, 127, 120, 352],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [342, 52, 239, 7, 77, 247, 275, 342, 155, 236],\n",
       " [268, 375, 343, 385, 343, 279, 54, 203, 375, 328],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [347, 347, 62, 347, 54, 281, 1, 52, 347, 45],\n",
       " [348, 348, 348, 348, 348, 171, 251, 348, 307, 170],\n",
       " [234, 349, 36, 83, 349, 192, 239, 274, 242, 317],\n",
       " [248, 350, 350, 350, 350, 350, 350, 350, 350, 350],\n",
       " [351, 62, 355, 101, 351, 297, 351, 113, 294, 237],\n",
       " [352, 134, 352, 0, 295, 134, 46, 363, 134, 269],\n",
       " [353, 202, 263, 0, 52, 353, 77, 263, 168, 0],\n",
       " [354, 354, 266, 354, 186, 354, 307, 265, 307, 354],\n",
       " [355, 355, 111, 93, 355, 355, 192, 237, 93, 237],\n",
       " [356, 356, 22, 103, 22, 208, 166, 363, 136, 70],\n",
       " [357, 357, 0, 193, 185, 357, 223, 198, 225, 176],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [52, 360, 360, 229, 231, 202, 348, 201, 229, 360],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [362, 154, 116, 66, 66, 77, 203, 62, 308, 178],\n",
       " [363, 319, 375, 375, 200, 312, 52, 237, 185, 52],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [366, 168, 366, 77, 168, 366, 334, 52, 334, 319],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [375, 268, 24, 343, 385, 204, 204, 54, 279, 375],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [377, 301, 336, 280, 291, 77, 291, 377, 237, 220],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [379, 379, 266, 3, 265, 307, 335, 325, 354, 307],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [316, 168, 32, 34, 328, 18, 184, 375, 77, 202],\n",
       " [382, 77, 262, 124, 336, 77, 299, 125, 77, 299],\n",
       " [382, 262, 337, 383, 296, 143, 141, 319, 138, 238],\n",
       " [328, 328, 384, 295, 111, 384, 384, 328, 124, 242],\n",
       " [328, 385, 375, 77, 200, 167, 111, 289, 295, 140]]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(0, len(titles))):\n",
    "    response = client.memory.query(\n",
    "        bank_id=bank_id,\n",
    "        query=[titles[i]],\n",
    "        params={\"max_chunks\": top_k},\n",
    "    )\n",
    "    res = [int(chunk.document_id) for chunk in response.chunks]\n",
    "    results.append(res)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(386, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(386, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(386, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "top_k = 3\n",
    "\n",
    "res = np.array(results)\n",
    "\n",
    "\n",
    "targets = np.array(doc_ids)\n",
    "display(targets.shape)\n",
    "targets = np.expand_dims(targets, axis=1)\n",
    "display(targets.shape)\n",
    "targets = targets == res\n",
    "display(targets.shape)\n",
    "targets = targets.astype(float)\n",
    "display(targets)\n",
    "\n",
    "targets = targets[:, :top_k]\n",
    "display(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.49740932642487046)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.sum(targets, axis=1)\n",
    "temp = np.divide(temp, top_k)\n",
    "precision = np.sum(temp) / len(temp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7979274611398963)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.logical_or.reduce(targets, axis=1)\n",
    "temp = temp.astype(float)\n",
    "recall = np.sum(temp) / len(temp)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7055267702936097)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = np.tile(np.arange(1, top_k + 1), (len(titles), 1)).astype(float)\n",
    "temp = np.copy(targets)\n",
    "for i in range(len(temp)):\n",
    "    first_true = True\n",
    "    for j in range(len(temp[i])):\n",
    "        if temp[i,j]:\n",
    "            if first_true:\n",
    "                first_true = False\n",
    "            else:\n",
    "                temp[i,j] = False\n",
    "reciprocal_ranks = temp / ranks\n",
    "reciprocal_ranks = np.sum(reciprocal_ranks) / len(temp)\n",
    "reciprocal_ranks = np.mean(reciprocal_ranks)\n",
    "reciprocal_ranks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
